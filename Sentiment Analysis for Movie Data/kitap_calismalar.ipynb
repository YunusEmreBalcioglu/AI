{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/ASUS/AI-ML/PycharmProjects/pythonProject/sentimentanalysisofIMDBmovie/movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a dataset\n",
    "\n",
    "target = df.pop('sentiment')\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    (df.values, target.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "ds_raw = ds_raw.shuffle(\n",
    "    50000, reshuffle_each_iteration=False)\n",
    "\n",
    "ds_raw_test = ds_raw.take(25000)\n",
    "ds_raw_train_valid = ds_raw.skip(25000)\n",
    "ds_raw_train = ds_raw_train_valid.take(20000)\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 87063\n"
     ]
    }
   ],
   "source": [
    "## Step 2: find unique tokens (words)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "except AttributeError:\n",
    "    tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "    \n",
    "token_counts = Counter()\n",
    "\n",
    "for example in ds_raw_train:\n",
    "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "    token_counts.update(tokens)\n",
    "    \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[544, 40, 223, 2166]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 3: encoding each unique token into integers\n",
    "\n",
    "try:\n",
    "    encoder = tfds.features.text.TokenTextEncoder(token_counts)\n",
    "except AttributeError:\n",
    "    encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "\n",
    "example_str = 'This is an example!'\n",
    "encoder.encode(example_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-A: define the function for transformation\n",
    "\n",
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "## Step 3-B: wrap the encode function to a TF Op.\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], \n",
    "                          Tout=(tf.int64, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: (109,)\n",
      "Sequence length: (177,)\n",
      "Sequence length: (281,)\n",
      "Sequence length: (199,)\n",
      "Sequence length: (124,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(124,), dtype=int64, numpy=\n",
       " array([   86,  9018, 12346, 16862,    40,    46,   653,  3385,     8,\n",
       "           46,    22,    71,   181,    33, 16863,   513,    17,     9,\n",
       "         3032,  2844,     8,   401, 16864, 15670,  4170,   319,   319,\n",
       "          337,   299,    51,    86,   837,    40, 16224, 16865, 16866,\n",
       "         2871,    44,   270,  1719,    14, 11589, 10498,   116,  1072,\n",
       "         1106,   223,   313,  4994,  2488,    16,    30,   483,    58,\n",
       "          268,   183,   604,   105,     9,  1842,  8139,  4170,   219,\n",
       "          299,   223,  2246,   545,   515,   972,   249,    46, 16867,\n",
       "           46,  3622,  3974,  9073,  4405,    46,  3032,  3142,   972,\n",
       "           14,    46,  6036,    44,     9, 16868,  2046,    14,    61,\n",
       "          299,    46,  8701,  1029,   139,    46,  3275,    14,    46,\n",
       "        16869,   214,  4891,   319,   319,   263,  2537,   299,  1183,\n",
       "          374,  4104,  3797,  8139,  4170,   299, 16870,  8519,    14,\n",
       "           13,   125,    44,   284,  4422, 13384,  1207], dtype=int64)>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = ds_raw_train.map(encode_map_fn)\n",
    "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "for example in ds_train.shuffle(1000).take(5):\n",
    "    print('Sequence length:', example[0].shape)\n",
    "    \n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual size: (236,)\n",
      "Individual size: (250,)\n",
      "Individual size: (193,)\n",
      "Individual size: (310,)\n",
      "Individual size: (143,)\n",
      "Individual size: (258,)\n",
      "Individual size: (214,)\n",
      "Individual size: (117,)\n",
      "Batch dimension: (4, 310)\n",
      "Batch dimension: (4, 258)\n"
     ]
    }
   ],
   "source": [
    "## Take a small subset\n",
    "\n",
    "ds_subset = ds_train.take(8)\n",
    "for example in ds_subset:\n",
    "    print('Individual size:', example[0].shape)\n",
    "\n",
    "## batching the datasets\n",
    "ds_batched = ds_subset.padded_batch(\n",
    "    4, padded_shapes=([-1], []))\n",
    "\n",
    "for batch in ds_batched:\n",
    "    print('Batch dimension:', batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## batching the datasets\n",
    "train_data = ds_train.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))\n",
    "\n",
    "valid_data = ds_valid.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = ds_test.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, 20, 6)             600       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600\n",
      "Trainable params: 600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=100,\n",
    "                    output_dim=6,\n",
    "                    input_length=20,\n",
    "                    name='embed-layer'))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f6fea6f47ae512550f0b8facdbd035a93e1dd89633f7bf2dd00a2502c71d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
